# 注意事项：
# 1. 数据存储于`/usr/local/dataflux-func/`目录下，
#    部署前应当保证目录已经存在。
#    参考命令：
#       sudo mkdir -p /usr/local/dataflux-func/{data,data/extra-python-packages,mysql,redis}
#
# 2. 使用`docker stack`即进行部署。
#    参考命令（假设本配置文件名为"docker-stack.yaml"）：
#       sudo docker stack deploy dataflux-func -c docker-stack.yaml
#
# 3. 如不需要内置 MySQL 组件，请删除 MYSQL 相关的内容块
# 4. 如不需要内置 Redis 组件，请删除 REDIS 相关的内容块
# 5. 如使用默认方式安装，请删除 WORKER MINI 相关的内容块
# 6. 如使用 mini 方式安装，请删除 WORKER DEFAULT 相关的内容块

version: '3.1'

services:
  # MYSQL START
  mysql:
    image: <MYSQL_IMAGE>
    labels:
      - mysql
    logging:
      driver: json-file
      options:
        max-size: 1m
        max-file: 10
    networks:
      - datafluxfunc
    volumes:
      - "<INSTALL_DIR>/mysql:/var/lib/mysql"
    environment:
      - "MYSQL_ROOT_PASSWORD=<MYSQL_PASSWORD>"
      - "MYSQL_DATABASE=dataflux_func"
    # command: --tls-version=TLSv1.2 --innodb-large-prefix=on --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci
    command: --tls-version=TLSv1.2 --innodb-large-prefix=on --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci --performance-schema=off --table-open-cache=400
  # MYSQL END

  # REDIS START
  redis:
    image: <REDIS_IMAGE>
    labels:
      - redis
    logging:
      driver: json-file
      options:
        max-size: 1m
        max-file: 10
    networks:
      - datafluxfunc
    volumes:
      - "<INSTALL_DIR>/redis:/data"
    command: --stop-writes-on-bgsave-error yes
  # REDIS END

  # WORKER DEFAULT START
  worker-0:
    image: <DATAFLUX_FUNC_IMAGE>
    deploy:
      replicas: 1
    labels:
      - worker-0
    volumes:
      - "<INSTALL_DIR>/data:/data"
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 10
    networks:
      - datafluxfunc
      - default
    environment:
      HOST_HOSTNAME: '{{.Node.Hostname}}' # 宿主机 hostname
      DFF__WORKER_CONCURRENCY: '2'        # 单 Worker 并发进程数
    command: ./run-worker-by-queue.sh 0

  worker-1-5:
    image: <DATAFLUX_FUNC_IMAGE>
    deploy:
      replicas: 1
    labels:
      - worker-1-5
    volumes:
      - "<INSTALL_DIR>/data:/data"
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 10
    networks:
      - datafluxfunc
      - default
    environment:
      HOST_HOSTNAME: '{{.Node.Hostname}}' # 宿主机 hostname
    command: ./run-worker-by-queue.sh 1 2 3 4 5

  worker-6:
    image: <DATAFLUX_FUNC_IMAGE>
    deploy:
      replicas: 1
    labels:
      - worker-6
    volumes:
      - "<INSTALL_DIR>/data:/data"
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 10
    networks:
      - datafluxfunc
      - default
    environment:
      HOST_HOSTNAME: '{{.Node.Hostname}}' # 宿主机 hostname
    command: ./run-worker-by-queue.sh 6

  worker-7:
    image: <DATAFLUX_FUNC_IMAGE>
    deploy:
      replicas: 1
    labels:
      - worker-7
    volumes:
      - "<INSTALL_DIR>/data:/data"
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 10
    networks:
      - datafluxfunc
      - default
    environment:
      HOST_HOSTNAME: '{{.Node.Hostname}}' # 宿主机 hostname
      DFF__WORKER_CONCURRENCY: '2'        # 单 Worker 并发进程数
    command: ./run-worker-by-queue.sh 7

  worker-8-9:
    image: <DATAFLUX_FUNC_IMAGE>
    deploy:
      replicas: 1
    labels:
      - worker-8-9
    volumes:
      - "<INSTALL_DIR>/data:/data"
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 10
    networks:
      - datafluxfunc
      - default
    environment:
      HOST_HOSTNAME: '{{.Node.Hostname}}' # 宿主机 hostname
    command: ./run-worker-by-queue.sh 8 9
  # WORKER DEFAULT END

  # WORKER MINI START
  worker:
    image: <DATAFLUX_FUNC_IMAGE>
    deploy:
      replicas: 2
    labels:
      - worker
    volumes:
      - "<INSTALL_DIR>/data:/data"
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 10
    networks:
      - datafluxfunc
      - default
    environment:
      HOST_HOSTNAME: '{{.Node.Hostname}}' # 宿主机 hostname
    command: ./run-worker.sh
  # WORKER MINI END

  beat:
    image: <DATAFLUX_FUNC_IMAGE>
    labels:
      - beat
    volumes:
      - "<INSTALL_DIR>/data:/data"
    logging:
      driver: json-file
      options:
        max-size: 1m
        max-file: 10
    networks:
      - datafluxfunc
      - default
    environment:
      HOST_HOSTNAME: '{{.Node.Hostname}}' # 宿主机 hostname
    command: ./run-beat.sh

  server:
    image: <DATAFLUX_FUNC_IMAGE>
    deploy:
      replicas: 1
    labels:
      - server
    volumes:
      - "<INSTALL_DIR>/data:/data"
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 10
    networks:
      - datafluxfunc
      - default
    environment:
      HOST_HOSTNAME: '{{.Node.Hostname}}' # 宿主机 hostname
    ports:
      - "<PORT>:8088"
    command: ./run-server.sh

networks:
  default:
    external:
      name: bridge
  datafluxfunc:
    driver: overlay

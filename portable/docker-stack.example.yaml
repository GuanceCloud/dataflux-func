# 注意事项：
# 1. 数据存储于`/usr/local/dataflux-func/`目录下，
#    部署前应当保证目录已经存在。
#    参考命令：
#       sudo mkdir -p /usr/local/dataflux-func/{data,data/extra-python-packages,mysql,redis}

# 2. 使用`docker stack`即进行部署。
#    参考命令（假设本配置文件名为"docker-stack.yaml"）：
#       sudo docker stack deploy dataflux-func -c docker-stack.yaml

# 3. 如不需要内置 MySQL 组件，请删除 MYSQL 相关的内容块
# 4. 如不需要内置 Redis 组件，请删除 REDIS 相关的内容块
# 5. 如使用默认方式安装，请删除 WORKER MINI 相关的内容块
# 6. 如使用 mini 方式安装，请删除 WORKER DEFAULT 相关的内容块
# 7. 当前版本各队列分配如下：
#     队列-0：系统任务
#     队列-1：用户函数（一般用途）
#     队列-2：用户函数（自动触发）
#     队列-3：用户函数（批处理）
#     队列-4：空闲
#     队列-5：Debug 运行
#     队列-6：消息订阅
#     队列-7：空闲 / 供观测云专用（一般用途）
#     队列-8：空闲 / 观测云专用（自动触发配置）
#     队列-9：空闲 / 观测云专用（自动触发配置-复杂任务）

version: '3.1'

services:
  # MYSQL START
  mysql:
    image: <MYSQL_IMAGE>
    labels:
      - mysql
    logging:
      driver: json-file
      options:
        max-size: 1m
        max-file: 10
    networks:
      - datafluxfunc
    volumes:
      - "<INSTALL_DIR>/mysql:/var/lib/mysql"
    environment:
      - "MYSQL_ROOT_PASSWORD=<MYSQL_PASSWORD>"
      - "MYSQL_DATABASE=dataflux_func"
    # command: --tls-version=TLSv1.2 --innodb-large-prefix=on --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci
    command: --tls-version=TLSv1.2 --innodb-large-prefix=on --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci --performance-schema=off --table-open-cache=400
  # MYSQL END

  # REDIS START
  redis:
    image: <REDIS_IMAGE>
    labels:
      - redis
    logging:
      driver: json-file
      options:
        max-size: 1m
        max-file: 10
    networks:
      - datafluxfunc
    volumes:
      - "<INSTALL_DIR>/redis:/data"
    command: --stop-writes-on-bgsave-error yes
  # REDIS END

  # WORKER DEFAULT START
  worker-0:
    image: <DATAFLUX_FUNC_IMAGE>
    deploy:
      replicas: 1
    labels:
      - worker-0
      - queue-0
      - system-task
      - queue-4
      - queue-7
      - queue-8
      - queue-9
      - reserved-for-guance
    volumes:
      - "<INSTALL_DIR>/data:/data"
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 10
    networks:
      - datafluxfunc
      - default
    environment:
      HOST_HOSTNAME: '{{.Node.Hostname}}' # 宿主机 hostname
      DFF__WORKER_CONCURRENCY: '2'        # 单 Worker 并发进程数
    command: ./run-worker-by-queue.sh 0 4 7 8 9

  worker-1:
    image: <DATAFLUX_FUNC_IMAGE>
    deploy:
      replicas: 1
    labels:
      - worker-1
      - queue-1
      - user-task-general
    volumes:
      - "<INSTALL_DIR>/data:/data"
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 10
    networks:
      - datafluxfunc
      - default
    environment:
      HOST_HOSTNAME: '{{.Node.Hostname}}'
    command: ./run-worker-by-queue.sh 1

  worker-2:
    image: <DATAFLUX_FUNC_IMAGE>
    deploy:
      replicas: 1
    labels:
      - worker-2
      - queue-2
      - user-task-crontab-schedule
    volumes:
      - "<INSTALL_DIR>/data:/data"
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 10
    networks:
      - datafluxfunc
      - default
    environment:
      HOST_HOSTNAME: '{{.Node.Hostname}}'
    command: ./run-worker-by-queue.sh 2

  worker-3:
    image: <DATAFLUX_FUNC_IMAGE>
    deploy:
      replicas: 1
    labels:
      - worker-3
      - queue-3
      - user-task-async-api
    volumes:
      - "<INSTALL_DIR>/data:/data"
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 10
    networks:
      - datafluxfunc
      - default
    environment:
      HOST_HOSTNAME: '{{.Node.Hostname}}'
    command: ./run-worker-by-queue.sh 3

  worker-5:
    image: <DATAFLUX_FUNC_IMAGE>
    deploy:
      replicas: 1
    labels:
      - worker-5
      - queue-5
      - debugger
    volumes:
      - "<INSTALL_DIR>/data:/data"
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 10
    networks:
      - datafluxfunc
      - default
    environment:
      HOST_HOSTNAME: '{{.Node.Hostname}}'
      DFF__WORKER_CONCURRENCY: '2'
    command: ./run-worker-by-queue.sh 5

  worker-6:
    image: <DATAFLUX_FUNC_IMAGE>
    deploy:
      replicas: 1
    labels:
      - worker-6
      - queue-6
      - user-task-sub
    volumes:
      - "<INSTALL_DIR>/data:/data"
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 10
    networks:
      - datafluxfunc
      - default
    environment:
      HOST_HOSTNAME: '{{.Node.Hostname}}'
    command: ./run-worker-by-queue.sh 6
  # WORKER DEFAULT END

  # WORKER MINI START
  worker:
    image: <DATAFLUX_FUNC_IMAGE>
    deploy:
      replicas: 2
    labels:
      - worker
    volumes:
      - "<INSTALL_DIR>/data:/data"
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 10
    networks:
      - datafluxfunc
      - default
    environment:
      HOST_HOSTNAME: '{{.Node.Hostname}}' # 宿主机 hostname
    command: ./run-worker.sh
  # WORKER MINI END

  beat:
    image: <DATAFLUX_FUNC_IMAGE>
    labels:
      - beat
    volumes:
      - "<INSTALL_DIR>/data:/data"
    logging:
      driver: json-file
      options:
        max-size: 1m
        max-file: 10
    networks:
      - datafluxfunc
      - default
    environment:
      HOST_HOSTNAME: '{{.Node.Hostname}}' # 宿主机 hostname
    command: ./run-beat.sh

  server:
    image: <DATAFLUX_FUNC_IMAGE>
    deploy:
      replicas: 1
    labels:
      - server
    volumes:
      - "<INSTALL_DIR>/data:/data"
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 10
    networks:
      - datafluxfunc
      - default
    environment:
      HOST_HOSTNAME: '{{.Node.Hostname}}' # 宿主机 hostname
    ports:
      - "<PORT>:8088"
    command: ./run-server.sh

networks:
  default:
    external:
      name: bridge
  datafluxfunc:
    driver: overlay
